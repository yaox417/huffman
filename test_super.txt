CS 5002 -- Final Project Instructions Proposals due 11/15 11PM (on Blackboard) Code and Writeups due Dec 6 11PMYour final project will be one large project in which you further explore one of the topics in this course with a C program of your own design. (Sorry, no group projects.) You can choose one of the following topics or propose your own -- though your proposed project must somehow expand on one of the topics we’ve covered (or that we will cover) in this course. Don’t go crazy now -- unless you plan to work on it over Thanksgiving, you’ll only have about two weeks. So, think of the right size as one of our normal programming assignments, x2. (Or one of the halves of a two-problem assignment, x4.) Some additional work besides that is the planning, which you will do this week. You’ll also need to submit in the end a brief writeup of what you did and how it differs from what you proposed. (More specific instructions for that will come later.)Extra credit is possible if you (a) go above and beyond and (b) you present your work on the last day of class. If I see you headed toward (a), I’ll invite you to do (b).Answer the following questions in your proposal (please use this numbering system for ease of reference):1.) Are you using one of the given “menu” options -- and if so, which one?If the answer to the previous question was “yes,” you can skip to number 4.2.) In general terms, what is your program going to do?3.) What concept from the course is this going to exercise or expand upon?4.) Give an example of how you imagine a user interacting with your program (as in, sample input and sample output) -- or, if there’s no user interaction, give a mockup of the kind of output you think your program will generate.5.) Is there anything you don’t know at the moment that you will need to learn to finish this project?6.) List any structs you think you will need to create for this project, and any data structures you think you will need to use or create.7.) Give a list of the signatures (return types and arguments) of the functions you think you will need to write for this project. You don’t have to completely predict what you will need, or exactly the right signatures, but you should have some idea as to the major tasks your code will try to accomplish. (Putting it all in main is not going to be a great strategy here.)8.) Give a timeline of major milestones for this project. (Assume you’ll be able to start the weekend after this proposal is due -- I’m hoping to get feedback to you by then.) You should have a milestone at least every week between when you start and the finish, though you’re welcome to take a break over Thanksgiving.A list of sample projects follows -- but you’re encouraged to use your imagination!
Puzzle Solver -- Implement a puzzle solver for “word ladders” or another small puzzle using breadth-first search. A word ladder is a puzzle where you try to change one word to another by changing one letter at a time, where all the words between are real words, using as few moves as possible. This is actually a search problem - represent the words as vertices and treat them as neighbors if they differ by one letter. Several other kinds of puzzles are really search problems. (Topics: Graphs, Search)Probabilistic Analysis -- Analyze the probabilities involved in one of your favorite games (that hasn’t already been analyzed on the Internet). There should be somewhat complex things going on, but any card game with a variety of meaningful combinations of cards is probably all right. You will turn in the code that calculates the probabilities, and include a short writeup of what it all means. (Topics: Probability)Queueing Simulation -- Try to understand supermarket checkout lines through simulations. How big is the advantage to the consumer for looking at all lines and picking the shortest, versus looking at just a few and picking the shortest of those, versus just picking the first line they randomly come across? You’ll turn in an analysis of your simulation along with the code. (Topics: Probability, Queues)Huffman Compression - Implement “Huffman coding,” which can compress input by creating new variable-length binary codes for the characters in a file. First, the counts of characters in a file are used to create a tree that represents new variable-length binary codes for each character. Then, the file is translated into this new binary representation, which uses fewer bits overall because the more frequent characters are given shorter codes. Bitwise operators are used to write the binary to a buffer, which is then written to a file. The same process in reverse can uncompress the file. (I will help you with this one. Topics: Binary, Trees)Random Number Library -- Create your own random number generator, which you can do by using a hash function on the same variable repeatedly to get new “pseudorandom” values. Use it to implement a library of functions for generating random results, such as dice rolls and card draws. Also run some experiments to make sure it really looks random. (Topics: Hash Tables, Probability)Six Degrees - Translate a text file of connections between people or things into a graph, and use breadth-first-search to find the shortest path of connections between them. (This is the same idea as “Six Degrees of Kevin Bacon” or Erdos numbers, if you’ve heard of either of those things.) If you’re tired of BFS you could try “iterative deepening depth-first search,” which is DFS but with an increasing maximum search depth (try everything 1 step away, then 2 steps away, then...) - it’s guaranteed to be optimal, like BFS. (Topics: Graphs, Search)ASCII Art Program - Create a program that lets the user easily draw lines of text characters onto a 2D grid, to create throwback ASCII-based art. Include at least one of a “Back” button powered by a stack and/or a “floodfill” command powered by DFS or BFS. (Topics: 2D arrays, stacks, search)Big Binary - Since every file on your computer is really a bunch of binary, and we could interpret any sequence of binary as one big number, every file on your computer has a number that uniquely describes it. Write a program that reads in a file and prints out the giant decimal number that represents the value of that file’s binary. (Topics: Binary, probably linked lists)
HTML Parser - Write a basic HTML parser that determines whether tags are being used correctly, and if they are, tries to emulate their effect in text. For example, if you see the bold tags <b> and </b>, you might put *asterisks* around that text. Since tags can be nested, you should have a stack to keep track of the open tags. (For extra credit, you can actually hook your HTML parser up to the Internet and download web pages, although you should expect to do some extra work to figure out how to connect.) (Topics: Stack, Strings/Arrays)I hope something here has either struck your fancy or started your imagination!
In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes".[1]

The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted.[2] However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.

Specifically, Huffman coding is optimal only if the probabilities of symbols are natural powers of 1/2. This is usually not the case. As an example, a symbol of probability 0.99 carries only log(1/0.99)=0.014 bits of information, but Huffman coding encodes each symbol separately and therefore the minimum length for each symbol is 1 bit. This sub-optimality is repaired in arithmetic coding and recent faster Asymmetric Numeral Systems family of entropy codings.

History
In 1951, David A. Huffman and his MIT information theory classmates were given the choice of a term paper or a final exam. The professor, Robert M. Fano, assigned a term paper on the problem of finding the most efficient binary code. Huffman, unable to prove any codes were the most efficient, was about to give up and start studying for the final when he hit upon the idea of using a frequency-sorted binary tree and quickly proved this method the most efficient.[3]

In doing so, Huffman outdid Fano, who had worked with information theory inventor Claude Shannon to develop a similar code. By building the tree from the bottom up instead of the top down, Huffman avoided the major flaw of the suboptimal Shannon-Fano coding.

Terminology
Huffman coding uses a specific method for choosing the representation for each symbol, resulting in a prefix code (sometimes called "prefix-free codes", that is, the bit string representing some particular symbol is never a prefix of the bit string representing any other symbol). Huffman coding is such a widespread method for creating prefix codes that the term "Huffman code" is widely used as a synonym for "prefix code" even when such a code is not produced by Huffman's algorithm.

Problem definition
Informal description
Given: A set of symbols and their weights (usually proportional to probabilities).
Find: A prefix-free binary code (a set of codewords) with minimum expected codeword length (equivalently, a tree with minimum weighted path length from the root).
Example
We give an example of the result of Huffman coding for a code with five characters and given weights. We will not verify that it minimizes L over all codes, but we will compute L and compare it to the Shannon entropy H of the given set of weights; the result is nearly optimal.

For any code that is biunique, meaning that the code is uniquely decodeable, the sum of the probability budgets across all symbols is always less than or equal to one. In this example, the sum is strictly equal to one; as a result, the code is termed a complete code. If this is not the case, you can always derive an equivalent code by adding extra symbols (with associated null probabilities), to make the code complete while keeping it biunique.

As defined by Shannon (1948), the information content h (in bits) of each symbol ai with non-null probability is h(a_{i})=\log _{2}{1 \over w_{i}}.
The entropy H (in bits) is the weighted sum, across all symbols ai with non-zero probability wi, of the information content of each symbol:
H(A)=\sum _{w_{i}>0}w_{i}h(a_{i})=\sum _{w_{i}>0}w_{i}\log _{2}{1 \over w_{i}}=-\sum _{w_{i}>0}w_{i}\log _{2}{w_{i}}.

